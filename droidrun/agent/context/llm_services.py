"""
LLMæœåŠ¡æ¨¡å— - å°è£…æ‰€æœ‰LLMè°ƒç”¨
"""
# æ ‡å‡†åº“å¯¼å…¥
import json
import re
from typing import Any, Dict, List, Optional
from droidrun.agent.utils.logging_utils import LoggingUtils

# ç¬¬ä¸‰æ–¹åº“å¯¼å…¥
from llama_index.core.llms.llm import LLM

class LLMServices:
    """LLMæœåŠ¡å°è£…ç±»"""
    
    def __init__(self, llm: LLM):
        self.llm = llm
        LoggingUtils.log_info("LLMServices", "LLMServices initialized")
    
    
#     def analyze_execution_anomaly(self, execution_log: List[Dict]) -> Dict[str, Any]:
#         """åˆ†ææ‰§è¡Œå¼‚å¸¸"""
#         try:
#             prompt = f"""
# åˆ†æä»¥ä¸‹æ‰§è¡Œæ—¥å¿—ï¼Œæ£€æµ‹æ˜¯å¦å­˜åœ¨å¼‚å¸¸ï¼š

# æ‰§è¡Œæ—¥å¿—: {json.dumps(execution_log, ensure_ascii=False, indent=2)}

# è¯·åˆ†æå¹¶è¿”å›JSONæ ¼å¼ï¼š
# {{
#     "has_anomaly": true/false,
#     "anomaly_type": "å¼‚å¸¸ç±»å‹",
#     "confidence": 0.0-1.0,
#     "description": "å¼‚å¸¸æè¿°",
#     "suggestion": "å»ºè®®çš„å›é€€ç­–ç•¥"
# }}
# """
#             response = self.llm.complete(prompt)
            
#             # è§£æJSONå“åº”
#             json_match = re.search(r'\{.*\}', response.text, re.DOTALL)
#             if json_match:
#                 analysis = json.loads(json_match.group())
#                 logger.info(f"ğŸ” LLM execution analysis completed")
#                 return analysis
#             else:
#                 logger.warning("Could not parse anomaly analysis from LLM response")
#                 return {
#                     "has_anomaly": False,
#                     "anomaly_type": "unknown",
#                     "confidence": 0.5,
#                     "description": "Could not parse LLM response",
#                     "suggestion": "Continue with current execution"
#                 }
                
#         except Exception as e:
#             logger.warning(f"LLM anomaly analysis failed: {e}")
#             return {
#                 "has_anomaly": False,
#                 "anomaly_type": "analysis_error",
#                 "confidence": 0.3,
#                 "description": f"Analysis failed: {str(e)}",
#                 "suggestion": "Continue with current execution"
#             }
    
    def extract_page_sequence(self, trajectory: Dict) -> List[Dict]:
        """ä»è½¨è¿¹ä¸­æå–é¡µé¢åºåˆ—"""
        try:
            prompt = f"""
ä»ä»¥ä¸‹æ‰§è¡Œè½¨è¿¹ä¸­æå–é¡µé¢è½¬æ¢åºåˆ—ï¼š

è½¨è¿¹æ•°æ®: {json.dumps(trajectory, ensure_ascii=False, indent=2)}

è¯·è¿”å›é¡µé¢åºåˆ—ï¼Œæ¯ä¸ªé¡µé¢åŒ…å«ï¼š
- page_name: é¡µé¢åç§°
- page_features: é¡µé¢ç‰¹å¾æè¿°
- transition_action: è½¬æ¢åŠ¨ä½œ
- ui_elements: å…³é”®UIå…ƒç´ 

è¿”å›JSONæ ¼å¼çš„æ•°ç»„ï¼š
"""
            response = self.llm.complete(prompt)
            
            # è§£æJSONå“åº”
            json_match = re.search(r'\[.*\]', response.text, re.DOTALL)
            if json_match:
                page_sequence = json.loads(json_match.group())
                LoggingUtils.log_info("LLMServices", "Extracted {count} pages from trajectory", count=len(page_sequence))
                return page_sequence
            else:
                LoggingUtils.log_warning("LLMServices", "Could not parse page sequence from LLM response")
                return []
                
        except Exception as e:
            LoggingUtils.log_warning("LLMServices", "Page sequence extraction failed: {error}", error=e)
            return []

    
    def _create_experience_summary(self, experience: Dict, index: int) -> Dict:
        """
        åˆ›å»ºç»éªŒçš„ç²¾ç®€æ‘˜è¦ï¼Œç”¨äºLLMé€‰æ‹©
        
        ä¼˜åŒ–ï¼šä»…ä¼ å…¥æ ¸å¿ƒå†³ç­–ä¿¡æ¯ï¼Œç§»é™¤å¤§æ•°æ®å­—æ®µï¼ˆaction_sequenceã€page_sequenceã€ui_statesï¼‰
        Tokenå‡å°‘ï¼š95%+ï¼Œä» ~50,000 é™è‡³ ~1,000
        """
        # ç»Ÿè®¡åŠ¨ä½œç±»å‹åˆ†å¸ƒ
        action_types = {}
        for action in experience.get("action_sequence", []):
            action_type = action.get("action", "unknown")
            action_types[action_type] = action_types.get(action_type, 0) + 1
        
        summary = {
            "index": index,  # ç”¨äºè¿”å›åŸå§‹ç»éªŒçš„ç´¢å¼•
            "goal": experience.get("goal", ""),
            "success": experience.get("success", False),
            "similarity_score": experience.get("similarity_score", 0.0),
            "metadata": {
                "steps": experience.get("metadata", {}).get("steps", 0),
                "execution_time": experience.get("metadata", {}).get("execution_time", 0),
                "is_hot_start": experience.get("metadata", {}).get("is_hot_start", False),
            },
            "statistics": {
                "action_count": len(experience.get("action_sequence", [])),
                "page_count": len(experience.get("page_sequence", [])),
                "action_types": action_types,
            }
        }
        return summary
    
    def select_best_experience(self, experiences: List[Dict], goal: str) -> Optional[Dict]:
        """
        é€‰æ‹©æœ€ä½³ç»éªŒ
        
        ä¼˜åŒ–ï¼šä½¿ç”¨ç²¾ç®€æ‘˜è¦ä»£æ›¿å®Œæ•´æ•°æ®ï¼Œå¤§å¹…å‡å°‘Tokenæ¶ˆè€—ï¼ˆ95%+ï¼‰
        """
        if not experiences:
            return None
        
        try:
            # åˆ›å»ºç²¾ç®€æ‘˜è¦ï¼ˆä»…åŒ…å«æ ¸å¿ƒå†³ç­–ä¿¡æ¯ï¼‰
            summaries = [self._create_experience_summary(exp, i) for i, exp in enumerate(experiences)]
            
            prompt = f"""
ä»ä»¥ä¸‹ç»éªŒæ‘˜è¦ä¸­é€‰æ‹©æœ€é€‚åˆæ–°ç›®æ ‡çš„æœ€ä½³ç»éªŒï¼š

æ–°ç›®æ ‡: {goal}

å¯ç”¨ç»éªŒæ‘˜è¦ï¼ˆå·²ä¼˜åŒ–ï¼Œä»…åŒ…å«æ ¸å¿ƒå†³ç­–ä¿¡æ¯ï¼‰:
{json.dumps(summaries, ensure_ascii=False, indent=2)}

è¯´æ˜ï¼š
- goal: ä»»åŠ¡ç›®æ ‡ï¼ˆæ ¸å¿ƒå†³ç­–ä¾æ®ï¼‰
- success: æ˜¯å¦æˆåŠŸæ‰§è¡Œ
- similarity_score: ä¸æ–°ç›®æ ‡çš„è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆ0-1ï¼‰
- metadata.steps: æ‰§è¡Œæ­¥éª¤æ•°
- metadata.execution_time: æ‰§è¡Œè€—æ—¶ï¼ˆç§’ï¼‰
- statistics.action_count: åŠ¨ä½œæ•°é‡
- statistics.action_types: åŠ¨ä½œç±»å‹åˆ†å¸ƒ

è¯·åˆ†æå¹¶è¿”å›JSONæ ¼å¼ï¼š
{{
    "best_experience_index": 0,
    "reason": "é€‰æ‹©ç†ç”±ï¼ˆé‡ç‚¹è¯´æ˜ä¸ºä½•è¯¥ç»éªŒæœ€é€‚åˆæ–°ç›®æ ‡ï¼‰",
    "confidence": 0.0-1.0
}}
"""
            LoggingUtils.log_info("LLMServices", "Selecting best from {count} experience summaries (optimized input, ~95% token reduction)", 
                                count=len(summaries))
            response = self.llm.complete(prompt)
            
            # è§£æJSONå“åº”
            json_match = re.search(r'\{.*\}', response.text, re.DOTALL)
            if json_match:
                selection = json.loads(json_match.group())
                best_index = selection.get("best_experience_index", 0)
                if 0 <= best_index < len(experiences):
                    LoggingUtils.log_info("LLMServices", "Selected best experience: {reason}", 
                                        reason=selection.get('reason', 'No reason provided'))
                    return experiences[best_index]
            
            # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›ç¬¬ä¸€ä¸ªç»éªŒ
            LoggingUtils.log_warning("LLMServices", "Could not parse best experience selection, using first experience")
            return experiences[0]
                
        except Exception as e:
            LoggingUtils.log_warning("LLMServices", "Best experience selection failed: {error}", error=e)
            return experiences[0] if experiences else None

    def detect_changed_actions(self, experience_goal: str, new_goal: str, actions: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ä½¿ç”¨LLMé€šç”¨è¯†åˆ«éœ€è¦å‚æ•°é€‚é…/å¾®å†·å¯åŠ¨çš„åŠ¨ä½œç´¢å¼•ï¼Œä¸¥ç¦é¢†åŸŸè¯ç¡¬ç¼–ç ã€‚

        çº¦æŸï¼š
        - è¿”å›çª—å£çº§ä¸€æ¬¡è§¦å‘ç‚¹ï¼ˆæ‰“å¼€/é€‰æ‹©/ç¡®è®¤é“¾æ¡ä»…è¿”å›ä¸€ä¸ªä»£è¡¨æ€§ç´¢å¼•ï¼‰
        - ä»…è¾“å‡ºJSONï¼Œå­—æ®µï¼š{"changed_indices": [int,...], "reasons": ["..."]}
        - è‹¥LLMä¸å¯ç”¨ï¼Œè¿”å›ç©ºåˆ—è¡¨ä½œä¸ºå…œåº•
        """
        try:
            # ä¸ºLLMæä¾›ç²¾ç®€ä¸”ç»“æ„åŒ–çš„åŠ¨ä½œè§†å›¾
            simplified_actions = []
            for i, a in enumerate(actions or []):
                simplified_actions.append({
                    "index": i,
                    "action": (a or {}).get("action") or (a or {}).get("name") or "",
                    "params": (a or {}).get("params") or (a or {}).get("parameters") or {},
                    "description": str((a or {}).get("description", ""))[:200]
                })

            prompt = f"""
ä½ æ˜¯ä¸€ä¸ªé€šç”¨çš„äººæœºæ“ä½œå·®å¼‚å¯¹é½å™¨ã€‚ç°æœ‰ä¸€ä¸ªå†å²ç»éªŒï¼ˆæ—§ç›®æ ‡ï¼‰ä¸ä¸€ä¸ªæ–°ç›®æ ‡ï¼Œä»¥åŠè¯¥ç»éªŒçš„åŠ¨ä½œåºåˆ—ï¼ˆæ¯æ­¥å«åŠ¨ä½œç±»å‹ã€ç®€è¿°ä¸å‚æ•°æ¦‚è§ˆï¼‰ã€‚
ä»»åŠ¡ï¼šæ¯”è¾ƒæ–°æ—§ç›®æ ‡ï¼Œæ‰¾å‡ºåŠ¨ä½œåºåˆ—ä¸­é‚£äº›å› ä¸ºç›®æ ‡å‚æ•°å˜åŒ–è€Œéœ€è¦è°ƒæ•´å‚æ•°æˆ–é‡åšçš„åŠ¨ä½œæ­¥éª¤ç´¢å¼•ã€‚åªè¿”å›è¿™äº›ç´¢å¼•å’Œå¯¹åº”çš„è¯¦ç»†å­ç›®æ ‡ã€‚

å…³é”®ç‚¹ï¼š
- åªåŸºäºæ–°æ—§ç›®æ ‡çš„è¯­ä¹‰å·®å¼‚å’ŒåŠ¨ä½œæè¿°è¿›è¡Œåˆ¤æ–­ã€‚
- é‡ç‚¹å…³æ³¨ä¸ç›®æ ‡å‚æ•°ç›´æ¥ç›¸å…³çš„åŠ¨ä½œï¼Œå¦‚è¾“å…¥æ–‡æœ¬ã€é€‰æ‹©æ—¥æœŸã€é€‰æ‹©é€‰é¡¹ç­‰ã€‚
- **ç‰¹åˆ«æ³¨æ„**ï¼šä»”ç»†æ£€æŸ¥æ–°ç›®æ ‡ä¸­æ˜¯å¦æœ‰å†å²åºåˆ—ä¸­**å®Œå…¨ä¸å­˜åœ¨**çš„æ“ä½œéœ€æ±‚ã€‚

**å˜æ›´ç±»å‹åˆ¤æ–­è§„åˆ™**ï¼š
1. **Changedï¼ˆä¿®æ”¹ï¼‰**ï¼šå†å²åŠ¨ä½œåºåˆ—ä¸­å·²å­˜åœ¨è¯¥åŠ¨ä½œï¼Œä½†å‚æ•°éœ€è¦è°ƒæ•´
   - ä¾‹å¦‚ï¼šå†å²ä¸­æœ‰"å¡«å†™å¼€å§‹æ—¥æœŸ2024å¹´1æœˆ1æ—¥"ï¼Œæ–°ä»»åŠ¡éœ€è¦"2025å¹´11æœˆ10æ—¥"
   - æ ‡è®°ä¸º changedï¼Œç´¢å¼•ä¸ºè¯¥åŠ¨ä½œåœ¨åºåˆ—ä¸­çš„ä½ç½®ï¼ˆå¦‚ index=3ï¼‰
   
2. **Addedï¼ˆæ–°å¢ï¼‰**ï¼šæ–°ä»»åŠ¡éœ€è¦çš„åŠ¨ä½œåœ¨å†å²åºåˆ—ä¸­å®Œå…¨ä¸å­˜åœ¨
   - ä¾‹å¦‚ï¼šå†å²åºåˆ—ä¸­åªæœ‰"å¡«å†™å¼€å§‹æ—¥æœŸ"ï¼Œä½†æ–°ä»»åŠ¡éœ€è¦"å¡«å†™å¼€å§‹æ—¥æœŸ"**å’Œ"å¡«å†™ç»“æŸæ—¥æœŸ"**
   - **é‡è¦è¯†åˆ«æ–¹æ³•**ï¼š
     * å¯¹æ¯”æ–°æ—§ç›®æ ‡ï¼Œè¯†åˆ«æ–°ç›®æ ‡ä¸­é¢å¤–çš„å‚æ•°æˆ–å­—æ®µ
     * æ£€æŸ¥å†å²åŠ¨ä½œåºåˆ—ä¸­æ˜¯å¦æœ‰å¯¹åº”çš„æ“ä½œ
     * å¦‚æœæ²¡æœ‰ï¼Œå°±æ˜¯ Added åŠ¨ä½œ
   - æ ‡è®°ä¸º addedï¼Œç´¢å¼•ä½¿ç”¨å°æ•°è¡¨ç¤ºæ’å…¥ä½ç½®
   - **æ’å…¥ä½ç½®è§„åˆ™**ï¼š
     * index=X.1 è¡¨ç¤ºæ’å…¥åˆ° index=X **ä¹‹å**
     * index=X.2 è¡¨ç¤ºæ’å…¥åˆ° index=X.1 ä¹‹å
     * **å…³é”®**ï¼šæ’å…¥ä½ç½®åº”è¯¥åŸºäº**ä¸šåŠ¡é€»è¾‘çš„é¡ºåº**ï¼Œè€Œä¸æ˜¯ç®€å•åœ°æ’å…¥åˆ°åºåˆ—æœ«å°¾
   - **æ’å…¥ä½ç½®ç¤ºä¾‹**ï¼š
     * å†å²åºåˆ—ï¼š[3]å¼€å§‹æ—¥æœŸ â†’ [4]ç¡®è®¤ â†’ [5]äº‹ç”±
     * æ–°å¢"ç»“æŸæ—¥æœŸ"åº”è¯¥åœ¨"å¼€å§‹æ—¥æœŸ"ä¹‹åã€"ç¡®è®¤"ä¹‹å‰
     * æ­£ç¡®ï¼šAdded [3.1] ç»“æŸæ—¥æœŸ âœ…ï¼ˆç´§è·Ÿå¼€å§‹æ—¥æœŸï¼‰
     * é”™è¯¯ï¼šAdded [4.1] ç»“æŸæ—¥æœŸ âŒï¼ˆåœ¨ç¡®è®¤ä¹‹åï¼Œé€»è¾‘é”™è¯¯ï¼‰
   - **å¸¸è§ Added åœºæ™¯**ï¼š
     * æ—¥æœŸèŒƒå›´ï¼šå†å²åªæœ‰"å¼€å§‹æ—¥æœŸ"ï¼Œæ–°ä»»åŠ¡éœ€è¦"å¼€å§‹æ—¥æœŸ+ç»“æŸæ—¥æœŸ"
     * é¢å¤–å­—æ®µï¼šå†å²åªå¡«å†™"äº‹ç”±"ï¼Œæ–°ä»»åŠ¡éœ€è¦"äº‹ç”±+å¤‡æ³¨"
     * å¤šé€‰é¡¹ï¼šå†å²é€‰æ‹©1ä¸ªé€‰é¡¹ï¼Œæ–°ä»»åŠ¡éœ€è¦é€‰æ‹©2ä¸ªé€‰é¡¹
   
3. **Removedï¼ˆåˆ é™¤ï¼‰**ï¼šå†å²åºåˆ—ä¸­å­˜åœ¨ä½†æ–°ä»»åŠ¡ä¸éœ€è¦çš„åŠ¨ä½œ
   - ä¾‹å¦‚ï¼šå†å²ä¸­æœ‰"é€‰æ‹©å®¡æ‰¹äºº"ï¼Œä½†æ–°ä»»åŠ¡ä¸éœ€è¦
   - æ ‡è®°ä¸º removedï¼Œç´¢å¼•ä¸ºè¯¥åŠ¨ä½œåœ¨å†å²åºåˆ—ä¸­çš„ä½ç½®

**é‡è¦**ï¼šä»”ç»†åŒºåˆ† Changed å’Œ Addedï¼
- å¦‚æœå†å²åºåˆ—ä¸­å·²æœ‰è¯¥åŠ¨ä½œï¼ˆå³ä½¿å‚æ•°ä¸åŒï¼‰ï¼Œåº”æ ‡è®°ä¸º Changedï¼Œè€Œä¸æ˜¯ Added
- åªæœ‰å†å²åºåˆ—ä¸­å®Œå…¨æ²¡æœ‰çš„åŠ¨ä½œæ‰æ ‡è®°ä¸º Added
- **è¯†åˆ« Added çš„å…³é”®æ­¥éª¤**ï¼š
  1. é€ä¸€å¯¹æ¯”æ–°æ—§ç›®æ ‡çš„æ¯ä¸ªå‚æ•°/å­—æ®µ
  2. å¯¹äºæ–°ç›®æ ‡ä¸­çš„æ¯ä¸ªå‚æ•°ï¼Œæ£€æŸ¥å†å²åºåˆ—æ˜¯å¦æœ‰å¯¹åº”æ“ä½œ
  3. å¦‚æœæ²¡æœ‰ï¼Œæ ‡è®°ä¸º Added
- **æ–°å¢åŠ¨ä½œçš„æ’å…¥ä½ç½®å¿…é¡»ç¬¦åˆä¸šåŠ¡é€»è¾‘**ï¼š
  * æ—¥æœŸç›¸å…³çš„åŠ¨ä½œåº”è¯¥è¿ç»­ï¼ˆå¼€å§‹æ—¥æœŸ â†’ ç»“æŸæ—¥æœŸ â†’ ç¡®è®¤ï¼‰
  * ä¸è¦æŠŠæ–°å¢åŠ¨ä½œæ’å…¥åˆ°ä¸ç›¸å…³çš„åŠ¨ä½œä¹‹å

**å®Œæ•´ç¤ºä¾‹**ï¼ˆé‡ç‚¹ï¼šæ—¥æœŸèŒƒå›´çš„ Added åœºæ™¯ï¼‰ï¼š
- æ—§ç›®æ ‡ï¼š"è¯·2025å¹´11æœˆ25æ—¥çš„å¹´ä¼‘å‡ï¼Œäº‹ç”±æ˜¯ä¼‘æ¯"
- æ–°ç›®æ ‡ï¼š"è¯·2025å¹´11æœˆ28åˆ°11æœˆ29è¿™ä¸¤å¤©çš„å¹´ä¼‘å‡ï¼Œäº‹ç”±æ˜¯ä¼‘æ¯"
- å†å²åŠ¨ä½œåºåˆ—ï¼š
  * [3] ç‚¹å‡»é€‰æ‹©"2025å¹´11æœˆ25æ—¥"ï¼ˆå¼€å§‹æ—¥æœŸï¼‰
  * [4] ç‚¹å‡»"ç¡®è®¤"æŒ‰é’®
  * [5] è¾“å…¥è¯·å‡äº‹ç”±"ä¼‘æ¯"
- æ­£ç¡®è¯†åˆ«ï¼š
  * Changed: [3]ï¼ˆå¼€å§‹æ—¥æœŸä»25æ”¹ä¸º28ï¼‰
  * Added: [3.1]ï¼ˆéœ€è¦æ–°å¢å¡«å†™ç»“æŸæ—¥æœŸ29ï¼Œæ’å…¥åœ¨å¼€å§‹æ—¥æœŸå’Œç¡®è®¤ä¹‹é—´ï¼‰
  * Changed: [5]ï¼ˆäº‹ç”±ä¸å˜ï¼Œä½†å¯èƒ½éœ€è¦é‡æ–°è¾“å…¥ï¼‰

**å­ç›®æ ‡æ ¼å¼è¦æ±‚**ï¼š
- ä½¿ç”¨ä¸­æ–‡ï¼Œ20-40å­—ï¼Œé¢å‘ä¸šåŠ¡ï¼Œæ¸…æ™°æè¿°è¦å®Œæˆä»€ä¹ˆæ“ä½œ
- é¿å…æŠ€æœ¯ç»†èŠ‚ï¼ˆå¦‚ç´¢å¼•ã€åæ ‡ç­‰ï¼‰
- ä½¿ç”¨"å®Œæˆ"ã€"å¡«å†™"ã€"é€‰æ‹©"ç­‰ä¸šåŠ¡åŠ¨è¯ï¼Œé¿å…"æ”¹ä¸º"ã€"ä¿®æ”¹"ã€"è°ƒæ•´"ç­‰æŠ€æœ¯åŠ¨è¯
- ç¤ºä¾‹ï¼š
  * âœ… "å®Œæˆå¼€å§‹æ—¥æœŸçš„å¡«å†™ï¼Œé€‰æ‹©2025å¹´11æœˆ10æ—¥"
  * âœ… "å®Œæˆç»“æŸæ—¥æœŸçš„å¡«å†™ï¼Œé€‰æ‹©2025å¹´11æœˆ15æ—¥"
  * âœ… "è¾“å…¥è¯·å‡äº‹ç”±ï¼šå»æµ·å—æ—…æ¸¸"
  * âŒ "å°†å¼€å§‹æ—¥æœŸæ”¹ä¸º2025å¹´11æœˆ10æ—¥"

ä»…è¿”å›JSONæ ¼å¼ï¼Œå­—æ®µå¦‚ä¸‹ï¼š
{{\n  "changed_indices": [æ•´æ•°æ•°ç»„],\n  "changed_reasons": [å­—ç¬¦ä¸²æ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ å¯¹åº”changed_indicesä¸­ç´¢å¼•çš„è¯¦ç»†å­ç›®æ ‡],
\n  "added_indices": [æµ®ç‚¹æ•°æ•°ç»„],\n  "added_reasons": [å­—ç¬¦ä¸²æ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ å¯¹åº”added_indicesä¸­ç´¢å¼•çš„è¯¦ç»†å­ç›®æ ‡],
\n  "removed_indices": [æ•´æ•°æ•°ç»„],\n  "removed_reasons": [å­—ç¬¦ä¸²æ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ å¯¹åº”removed_indicesä¸­ç´¢å¼•çš„è¯¦ç»†å­ç›®æ ‡],
\n}}
ä¸è¦è¾“å‡ºä»»ä½•å…¶ä»–æ–‡å­—ã€‚

æ—§ç›®æ ‡ï¼š{experience_goal}
æ–°ç›®æ ‡ï¼š{new_goal}
åŠ¨ä½œåºåˆ—ï¼ˆç²¾ç®€ï¼‰ï¼š{json.dumps(simplified_actions, ensure_ascii=False)}
"""
            # logger.info(f"[LLM][detect_changed_actions] Prompt:\n{prompt}")
            rsp = self.llm.complete(prompt)
            text = getattr(rsp, 'text', str(rsp))
            LoggingUtils.log_debug("LLMServices", "Detect changed actions response: {text}", text=text)
            
            # è§£æä¸¥æ ¼JSON
            m = re.search(r'\{[\s\S]*\}$', text.strip())
            data = json.loads(m.group()) if m else json.loads(text)
            
            # è§£æå˜æ›´åŠ¨ä½œ
            changed_indices_raw = data.get("changed_indices", [])
            changed_reasons_raw = data.get("changed_reasons", data.get("reasons", []))  # å…¼å®¹æ—§å­—æ®µå
            
            # è§£ææ–°å¢åŠ¨ä½œ
            added_indices_raw = data.get("added_indices", [])
            added_reasons_raw = data.get("added_reasons", [])
            
            # è§£æåˆ é™¤åŠ¨ä½œ
            removed_indices_raw = data.get("removed_indices", [])
            removed_reasons_raw = data.get("removed_reasons", [])
            
            # æ·»åŠ INFOçº§åˆ«æ—¥å¿—æ˜¾ç¤ºLLMè¯†åˆ«ç»“æœ
            LoggingUtils.log_info("LLMServices", 
                                "ğŸ” LLM detected: changed={changed}, added={added}, removed={removed}", 
                                changed=len(changed_indices_raw), 
                                added=len(added_indices_raw),
                                removed=len(removed_indices_raw))
            
            # è§„èŒƒåŒ–å˜æ›´åŠ¨ä½œç´¢å¼•ä¸ºæ•´æ•°
            norm_changed_indices: List[int] = []
            for i in changed_indices_raw:
                try:
                    ii = int(str(i))
                    norm_changed_indices.append(ii)
                except Exception:
                    continue
            
            # è§„èŒƒåŒ–æ–°å¢åŠ¨ä½œç´¢å¼•ä¸ºæµ®ç‚¹æ•°
            norm_added_indices: List[float] = []
            for i in added_indices_raw:
                try:
                    ff = float(str(i))
                    norm_added_indices.append(ff)
                except Exception:
                    continue
            
            # è§„èŒƒåŒ–åˆ é™¤åŠ¨ä½œç´¢å¼•ä¸ºæ•´æ•°
            norm_removed_indices: List[int] = []
            for i in removed_indices_raw:
                try:
                    ii = int(str(i))
                    norm_removed_indices.append(ii)
                except Exception:
                    continue
            
            # æ„é€  index->reason å¯¹é½è¡¨ï¼ˆå˜æ›´åŠ¨ä½œï¼‰
            index_reasons = []
            for pos, idx in enumerate(norm_changed_indices):
                reason = changed_reasons_raw[pos] if pos < len(changed_reasons_raw) else ""
                index_reasons.append({"index": idx, "reason": str(reason), "type": "changed"})
            
            # æ·»åŠ æ–°å¢åŠ¨ä½œåˆ°å¯¹é½è¡¨
            for pos, idx in enumerate(norm_added_indices):
                reason = added_reasons_raw[pos] if pos < len(added_reasons_raw) else ""
                index_reasons.append({"index": idx, "reason": str(reason), "type": "added"})
            
            # æ·»åŠ åˆ é™¤åŠ¨ä½œåˆ°å¯¹é½è¡¨
            for pos, idx in enumerate(norm_removed_indices):
                reason = removed_reasons_raw[pos] if pos < len(removed_reasons_raw) else ""
                index_reasons.append({"index": idx, "reason": str(reason), "type": "removed"})
            
            # ä¾›æ—§é€»è¾‘ä½¿ç”¨çš„å»é‡æ’åºé›†åˆ
            changed_sorted = sorted(set(norm_changed_indices))
            
            return {
                "changed_indices": changed_sorted, 
                "index_reasons": index_reasons,
                "added_indices": norm_added_indices,
                "removed_indices": norm_removed_indices,
                "reasons": changed_reasons_raw  # ä¿æŒå‘åå…¼å®¹
            }
        except Exception:
            LoggingUtils.log_warning("LLMServices", "Detect changed actions: LLMè§£æå¤±è´¥ï¼Œè¿”å›ç©ºé›†åˆä½œä¸ºå…œåº•")
            return {"changed_indices": []}

    def generate_micro_goal(self, action: Dict[str, Any], diffs: Dict[str, Any], new_goal: str) -> str:
        """ä½¿ç”¨LLMç”Ÿæˆé€šç”¨å¾®å†·å¯åŠ¨å­ç›®æ ‡ï¼š
        - å•å¥ä¸­æ–‡ï¼Œé¢å‘ä¸šåŠ¡ã€å¯æ‰§è¡Œ
        - ç¦æ­¢å‡ºç°ç´¢å¼•/åæ ‡/èµ„æºIDç­‰å®ç°ç»†èŠ‚
        - èšç„¦å½“å‰å­é˜¶æ®µï¼Œé¿å…æ¦‚æ‹¬å…¨æµç¨‹
        - ä¸¥ç¦ä½¿ç”¨ä»»ä½•é¢†åŸŸè¯ç¡¬ç¼–ç ï¼ˆæœ¬å‡½æ•°å†…ä¸å†™æ­»ä»»ä½•å…³é”®è¯ï¼‰
        """
        name = (action or {}).get("action") or (action or {}).get("name") or ""
        desc = str((action or {}).get("description", ""))

        # å°† diffs å‹ç¼©ä¸ºè‡ªç„¶è¯­è¨€æ‘˜è¦ä¾›LLMå‚è€ƒï¼ˆä¸å¼•å…¥é¢†åŸŸè¯åˆ¤æ–­ï¼Œä»…é€ä¼ ï¼‰
        try:
            diffs_text = json.dumps(diffs, ensure_ascii=False)
        except Exception:
            diffs_text = "{}"

        try:
            prompt = f"""
ä½ æ˜¯é€šç”¨çš„äººæœºäº¤äº’å­ç›®æ ‡ç”Ÿæˆå™¨ã€‚åŸºäºç»™å®šåŠ¨ä½œçš„è‡ªç„¶è¯­è¨€æè¿°ã€æ•´ä½“æ–°ä»»åŠ¡ç›®æ ‡ï¼Œä»¥åŠä¸¤è€…ä¹‹é—´çš„è¯­ä¹‰å·®å¼‚æ‘˜è¦ï¼Œä¸ºå½“å‰åŠ¨ä½œæ‰€åœ¨çš„å­æµç¨‹ç”Ÿæˆä¸€ä¸ª"å¯ç›´æ¥æ‰§è¡Œçš„ã€é¢å‘ä¸šåŠ¡çš„ç›®æ ‡æè¿°"ã€‚

è¦æ±‚ï¼š
- è¾“å‡ºä¸­æ–‡ï¼Œ20-40å­—å†…ï¼›
- ç¦æ­¢å‡ºç°ç´¢å¼•ã€åæ ‡ã€èµ„æºIDç­‰å®ç°ç»†èŠ‚ï¼›
- åªèšç„¦å½“å‰å­é˜¶æ®µçš„å¯å®Œæˆç›®æ ‡ï¼Œé¿å…è¦†ç›–æ•´ä¸ªæµç¨‹ï¼›
- ç”¨è¯ä¸­ç«‹ï¼Œé¿å…ä¾èµ–ä»»ä½•ç‰¹å®šåº”ç”¨/å­—æ®µåï¼›
- **å…³é”®**ï¼šä½¿ç”¨"å®Œæˆ"ã€"å¡«å†™"ã€"é€‰æ‹©"ç­‰ä¸šåŠ¡åŠ¨è¯ï¼Œé¿å…ä½¿ç”¨"æ”¹ä¸º"ã€"ä¿®æ”¹"ã€"è°ƒæ•´"ç­‰æŠ€æœ¯åŠ¨è¯
  * âœ… æ­£ç¡®ç¤ºä¾‹ï¼š"å®Œæˆç»“æŸæ—¥æœŸçš„å¡«å†™ï¼Œé€‰æ‹©2025å¹´11æœˆ15æ—¥"
  * âœ… æ­£ç¡®ç¤ºä¾‹ï¼š"åœ¨æ—¥æœŸé€‰æ‹©å™¨ä¸­é€‰æ‹©2025å¹´11æœˆ15æ—¥å¹¶ç¡®è®¤"
  * âŒ é”™è¯¯ç¤ºä¾‹ï¼š"å°†ç»“æŸæ—¥æœŸæ”¹ä¸º2025å¹´11æœˆ15æ—¥"
  * âŒ é”™è¯¯ç¤ºä¾‹ï¼š"ä¿®æ”¹æ—¥æœŸä¸º2025å¹´11æœˆ15æ—¥"
- å¦‚æœæ¶‰åŠæ—¥æœŸï¼Œä½¿ç”¨å®Œæ•´å¹´æœˆæ—¥æ ¼å¼

åŠ¨ä½œæè¿°ï¼š{desc or name}
æ–°ä»»åŠ¡ç›®æ ‡ï¼š{new_goal}
å·®å¼‚æ‘˜è¦ï¼ˆä¾›å‚è€ƒï¼‰ï¼š{diffs_text}
"""
            LoggingUtils.log_debug("LLMServices", "Generate micro goal prompt: {prompt}", prompt=prompt)
            rsp = self.llm.complete(prompt)
            text = getattr(rsp, 'text', str(rsp)).strip().splitlines()[0]
            LoggingUtils.log_debug("LLMServices", "Generate micro goal text: {text}", text=text)
            # å…œåº•ï¼šè‹¥è¿”å›ç©ºæˆ–åŒ…å«æ˜æ˜¾å®ç°ç»†èŠ‚ï¼Œé€€å›åˆ°æ›´æ³›åŒ–çš„å­é˜¶æ®µè¡¨è¾¾
            if not text:
                raise ValueError("empty micro goal")
            if any(k in text for k in ("index", "åæ ‡", "resource-id", "id=", "@id/")):
                raise ValueError("goal contains low-level detail")
            return text
        except Exception:
            # æœ€å°å…œåº•ï¼šè¿”å›åŠ¨ä½œæè¿°æˆ–ææ³›åŒ–çŸ­å¥ï¼Œä¿æŒé€šç”¨
            return desc or "å®Œæˆè¯¥çª—å£çš„å½“å‰å­é˜¶æ®µ"
    
#     def generate_fallback_strategy(self, anomaly_type: str, anomaly_details: Dict) -> str:
#         """ç”Ÿæˆå›é€€ç­–ç•¥"""
#         try:
#             prompt = f"""
# åŸºäºä»¥ä¸‹å¼‚å¸¸ä¿¡æ¯ï¼Œç”Ÿæˆå…·ä½“çš„å›é€€ç­–ç•¥ï¼š

# å¼‚å¸¸ç±»å‹: {anomaly_type}
# å¼‚å¸¸è¯¦æƒ…: {json.dumps(anomaly_details, ensure_ascii=False, indent=2)}

# è¯·æä¾›å…·ä½“çš„å›é€€ç­–ç•¥ï¼ŒåŒ…æ‹¬ï¼š
# 1. å›é€€æ­¥éª¤
# 2. å‚æ•°è°ƒæ•´
# 3. é¢„æœŸç»“æœ

# å›é€€ç­–ç•¥ï¼š
# """
#             response = self.llm.complete(prompt)
#             strategy = response.text.strip()
#             logger.info(f"ğŸ”„ Generated fallback strategy for {anomaly_type}")
#             return strategy
            
#         except Exception as e:
#             logger.warning(f"Fallback strategy generation failed: {e}")
#             return f"Default fallback strategy for {anomaly_type}"
    
#     def validate_ui_state(self, ui_state: Dict, expected_elements: List[str]) -> Dict[str, Any]:
#         """éªŒè¯UIçŠ¶æ€"""
#         try:
#             prompt = f"""
# éªŒè¯å½“å‰UIçŠ¶æ€æ˜¯å¦åŒ…å«æœŸæœ›çš„å…ƒç´ ï¼š

# å½“å‰UIçŠ¶æ€: {json.dumps(ui_state, ensure_ascii=False, indent=2)}
# æœŸæœ›å…ƒç´ : {expected_elements}

# è¯·è¿”å›JSONæ ¼å¼ï¼š
# {{
#     "is_valid": true/false,
#     "found_elements": ["æ‰¾åˆ°çš„å…ƒç´ "],
#     "missing_elements": ["ç¼ºå¤±çš„å…ƒç´ "],
#     "confidence": 0.0-1.0,
#     "suggestion": "å»ºè®®"
# }}
# """
#             response = self.llm.complete(prompt)
            
#             # è§£æJSONå“åº”
#             json_match = re.search(r'\{.*\}', response.text, re.DOTALL)
#             if json_match:
#                 validation = json.loads(json_match.group())
#                 logger.info(f"âœ… UI state validation completed")
#                 return validation
#             else:
#                 logger.warning("Could not parse UI validation from LLM response")
#                 return {
#                     "is_valid": False,
#                     "found_elements": [],
#                     "missing_elements": expected_elements,
#                     "confidence": 0.3,
#                     "suggestion": "Could not validate UI state"
#                 }
                
#         except Exception as e:
#             logger.warning(f"UI state validation failed: {e}")
#             return {
#                 "is_valid": False,
#                 "found_elements": [],
#                 "missing_elements": expected_elements,
#                 "confidence": 0.3,
#                 "suggestion": f"Validation failed: {str(e)}"
#             }