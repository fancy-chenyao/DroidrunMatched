"""
æ‰§è¡Œç›‘æ§ç³»ç»Ÿ - ç›‘æ§æ‰§è¡Œè¿‡ç¨‹ï¼Œæ£€æµ‹å¼‚å¸¸ï¼Œè§¦å‘å›é€€
"""
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from enum import Enum
import time
import logging
import json
import re

logger = logging.getLogger("droidrun")

class MonitorStatus(Enum):
    NORMAL = "normal"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"

@dataclass
class MonitorResult:
    status: MonitorStatus
    message: str
    confidence: float
    fallback_needed: bool
    fallback_type: Optional[str] = None
    details: Optional[Dict[str, Any]] = None

class ExecutionMonitor:
    """æ‰§è¡Œç›‘æ§å™¨"""
    
    def __init__(self, llm=None):
        self.llm = llm
        self.execution_history: List[Dict] = []
        self.performance_metrics: Dict[str, Any] = {}
        self.step_start_time: Optional[float] = None
        self.consecutive_failures: int = 0
        self.max_consecutive_failures: int = 3
        
        logger.info("ğŸ” ExecutionMonitor initialized")
    
    def start_step_monitoring(self, step_data: Dict):
        """å¼€å§‹ç›‘æ§å•ä¸ªæ­¥éª¤"""
        self.step_start_time = time.time()
        self.execution_history.append({
            "step": len(self.execution_history) + 1,
            "start_time": self.step_start_time,
            "data": step_data
        })
        logger.debug(f"ğŸ” Started monitoring step {len(self.execution_history)}")
    
    def monitor_step(self, step_data: Dict) -> MonitorResult:
        """ç›‘æ§å•ä¸ªæ‰§è¡Œæ­¥éª¤"""
        try:
            # è®¡ç®—æ‰§è¡Œæ—¶é—´ - ä¿®å¤ï¼šä½¿ç”¨å½“å‰æ—¶é—´è€Œä¸æ˜¯step_start_time
            current_time = time.time()
            execution_time = current_time - self.step_start_time if self.step_start_time else 0
            
            # å¦‚æœè¿™æ˜¯ä»»åŠ¡å®Œæˆçš„æƒ…å†µï¼Œä¸è¿›è¡Œè¶…æ—¶æ£€æŸ¥
            if step_data.get("success", False) and step_data.get("steps", 0) > 10:
                logger.info(f"ğŸ¯ Task completed successfully with {step_data.get('steps', 0)} steps, skipping timeout check")
                return MonitorResult(
                    status=MonitorStatus.NORMAL,
                    message="Task completed successfully",
                    confidence=1.0,
                    fallback_needed=False
                )
            
            # åŸºæœ¬æ£€æŸ¥
            basic_result = self._check_basic_metrics(step_data, execution_time)
            if basic_result.fallback_needed:
                return basic_result
            
            # ä½¿ç”¨LLMè¿›è¡Œæ·±åº¦åˆ†æ
            if self.llm:
                llm_result = self._llm_analyze_step(step_data, execution_time)
                if llm_result.fallback_needed:
                    return llm_result
            
            # æ›´æ–°æ€§èƒ½æŒ‡æ ‡
            self._update_performance_metrics(step_data, execution_time)
            
            return MonitorResult(
                status=MonitorStatus.NORMAL,
                message="Step execution normal",
                confidence=0.9,
                fallback_needed=False
            )
            
        except Exception as e:
            logger.error(f"Error in step monitoring: {e}")
            return MonitorResult(
                status=MonitorStatus.ERROR,
                message=f"Monitoring error: {str(e)}",
                confidence=0.5,
                fallback_needed=True,
                fallback_type="monitoring_error"
            )
    
    def _check_basic_metrics(self, step_data: Dict, execution_time: float) -> MonitorResult:
        """åŸºæœ¬æŒ‡æ ‡æ£€æŸ¥"""
        # æ£€æŸ¥æ‰§è¡Œæ—¶é—´ - å¯¹äºLLMè°ƒç”¨ï¼Œ180ç§’å†…éƒ½æ˜¯æ­£å¸¸çš„ï¼ˆå¢åŠ è¶…æ—¶æ—¶é—´ï¼‰
        # ä¿®å¤ï¼šåªæœ‰åœ¨å•æ­¥æ‰§è¡Œæ—¶é—´è¿‡é•¿æ—¶æ‰è§¦å‘è¶…æ—¶ï¼Œè€Œä¸æ˜¯ç´¯è®¡æ—¶é—´
        if execution_time > 180:  # è¶…è¿‡180ç§’è®¤ä¸ºå¼‚å¸¸
            return MonitorResult(
                status=MonitorStatus.WARNING,
                message=f"Step execution time too long: {execution_time:.2f}s",
                confidence=0.8,
                fallback_needed=True,
                fallback_type="timeout"
            )
        
        # æ£€æŸ¥è¿ç»­å¤±è´¥
        if step_data.get("success", True) == False:
            self.consecutive_failures += 1
            if self.consecutive_failures >= self.max_consecutive_failures:
                return MonitorResult(
                    status=MonitorStatus.CRITICAL,
                    message=f"Too many consecutive failures: {self.consecutive_failures}",
                    confidence=0.9,
                    fallback_needed=True,
                    fallback_type="consecutive_failures"
                )
        else:
            self.consecutive_failures = 0
        
        # æ£€æŸ¥æ­¥éª¤æ•°é‡
        if len(self.execution_history) > 20:  # è¶…è¿‡20æ­¥è®¤ä¸ºå¯èƒ½æœ‰é—®é¢˜
            return MonitorResult(
                status=MonitorStatus.WARNING,
                message=f"Too many steps executed: {len(self.execution_history)}",
                confidence=0.7,
                fallback_needed=True,
                fallback_type="too_many_steps"
            )
        
        return MonitorResult(
            status=MonitorStatus.NORMAL,
            message="Basic checks passed",
            confidence=0.8,
            fallback_needed=False
        )
    
    def _llm_analyze_step(self, step_data: Dict, execution_time: float) -> MonitorResult:
        """ä½¿ç”¨LLMåˆ†ææ­¥éª¤æ‰§è¡Œ"""
        try:
            prompt = f"""
åˆ†æä»¥ä¸‹æ‰§è¡Œæ­¥éª¤æ˜¯å¦å­˜åœ¨å¼‚å¸¸ï¼š

æ­¥éª¤æ•°æ®: {step_data}
æ‰§è¡Œæ—¶é—´: {execution_time:.2f}ç§’
å†å²æ­¥éª¤æ•°: {len(self.execution_history)}

æ³¨æ„ï¼š
- å¯¹äºç§»åŠ¨åº”ç”¨æ“ä½œï¼Œ10-20ä¸ªæ­¥éª¤æ˜¯æ­£å¸¸çš„
- åªæœ‰åœ¨æ­¥éª¤æ•°å°‘äº3ä¸ªæˆ–è¶…è¿‡50ä¸ªæ—¶æ‰è€ƒè™‘å¼‚å¸¸
- ä»»åŠ¡æˆåŠŸå®Œæˆï¼ˆå¦‚æ˜¾ç¤º"æäº¤æˆåŠŸ"ï¼‰ä¸åº”è¢«è§†ä¸ºå¼‚å¸¸

è¯·åˆ†æå¹¶è¿”å›JSONæ ¼å¼ï¼š
{{
    "has_anomaly": true/false,
    "anomaly_type": "ç±»å‹",
    "confidence": 0.0-1.0,
    "suggestion": "å»ºè®®"
}}
"""
            response = self.llm.complete(prompt)
            
            # è§£æLLMå“åº”
            json_match = re.search(r'\{.*\}', response.text, re.DOTALL)
            if json_match:
                analysis = json.loads(json_match.group())
                
                if analysis.get("has_anomaly", False):
                    return MonitorResult(
                        status=MonitorStatus.WARNING,
                        message=f"LLM detected anomaly: {analysis.get('anomaly_type', 'unknown')}",
                        confidence=analysis.get("confidence", 0.7),
                        fallback_needed=True,
                        fallback_type=analysis.get("anomaly_type", "llm_detected"),
                        details=analysis
                    )
            
            return MonitorResult(
                status=MonitorStatus.NORMAL,
                message="LLM analysis passed",
                confidence=0.8,
                fallback_needed=False
            )
            
        except Exception as e:
            logger.warning(f"LLM analysis failed: {e}")
            return MonitorResult(
                status=MonitorStatus.NORMAL,
                message="LLM analysis skipped",
                confidence=0.5,
                fallback_needed=False
            )
    
    def detect_anomaly(self, execution_log: List[Dict]) -> MonitorResult:
        """æ£€æµ‹æ‰§è¡Œå¼‚å¸¸"""
        try:
            if not execution_log:
                return MonitorResult(
                    status=MonitorStatus.NORMAL,
                    message="No execution log to analyze",
                    confidence=1.0,
                    fallback_needed=False
                )
            
            # åŸºæœ¬ç»Ÿè®¡æ£€æŸ¥
            success_count = sum(1 for step in execution_log if step.get("success", False))
            total_steps = len(execution_log)
            success_rate = success_count / total_steps if total_steps > 0 else 0
            
            if success_rate < 0.5:  # æˆåŠŸç‡ä½äº50%
                return MonitorResult(
                    status=MonitorStatus.ERROR,
                    message=f"Low success rate: {success_rate:.2%}",
                    confidence=0.9,
                    fallback_needed=True,
                    fallback_type="low_success_rate"
                )
            
            # ä½¿ç”¨LLMè¿›è¡Œæ·±åº¦åˆ†æ
            if self.llm:
                return self._llm_analyze_execution(execution_log)
            
            return MonitorResult(
                status=MonitorStatus.NORMAL,
                message="Execution analysis passed",
                confidence=0.8,
                fallback_needed=False
            )
            
        except Exception as e:
            logger.error(f"Error in anomaly detection: {e}")
            return MonitorResult(
                status=MonitorStatus.ERROR,
                message=f"Anomaly detection error: {str(e)}",
                confidence=0.5,
                fallback_needed=True,
                fallback_type="detection_error"
            )
    
    def _llm_analyze_execution(self, execution_log: List[Dict]) -> MonitorResult:
        """ä½¿ç”¨LLMåˆ†ææ•´ä¸ªæ‰§è¡Œè¿‡ç¨‹"""
        try:
            prompt = f"""
åˆ†æä»¥ä¸‹æ‰§è¡Œæ—¥å¿—ï¼Œæ£€æµ‹æ˜¯å¦å­˜åœ¨å¼‚å¸¸ï¼š

æ‰§è¡Œæ—¥å¿—: {execution_log}

è¯·åˆ†æå¹¶è¿”å›JSONæ ¼å¼ï¼š
{{
    "has_anomaly": true/false,
    "anomaly_type": "å¼‚å¸¸ç±»å‹",
    "confidence": 0.0-1.0,
    "description": "å¼‚å¸¸æè¿°",
    "suggestion": "å»ºè®®çš„å›é€€ç­–ç•¥"
}}
"""
            response = self.llm.complete(prompt)
            
            # è§£æLLMå“åº”
            json_match = re.search(r'\{.*\}', response.text, re.DOTALL)
            if json_match:
                analysis = json.loads(json_match.group())
                
                if analysis.get("has_anomaly", False):
                    return MonitorResult(
                        status=MonitorStatus.ERROR,
                        message=f"LLM detected execution anomaly: {analysis.get('description', 'unknown')}",
                        confidence=analysis.get("confidence", 0.8),
                        fallback_needed=True,
                        fallback_type=analysis.get("anomaly_type", "llm_detected"),
                        details=analysis
                    )
            
            return MonitorResult(
                status=MonitorStatus.NORMAL,
                message="LLM execution analysis passed",
                confidence=0.8,
                fallback_needed=False
            )
            
        except Exception as e:
            logger.warning(f"LLM execution analysis failed: {e}")
            return MonitorResult(
                status=MonitorStatus.NORMAL,
                message="LLM execution analysis skipped",
                confidence=0.5,
                fallback_needed=False
            )
    
    def suggest_fallback(self, anomaly: MonitorResult) -> str:
        """å»ºè®®å›é€€ç­–ç•¥"""
        fallback_strategies = {
            "timeout": "é‡æ–°å¼€å§‹æ‰§è¡Œï¼Œä½¿ç”¨æ›´çŸ­çš„è¶…æ—¶æ—¶é—´",
            "consecutive_failures": "å›é€€åˆ°å†·å¯åŠ¨æ¨¡å¼ï¼Œé‡æ–°è§„åˆ’ä»»åŠ¡",
            "too_many_steps": "ç®€åŒ–ä»»åŠ¡ç›®æ ‡ï¼Œåˆ†è§£ä¸ºæ›´å°çš„æ­¥éª¤",
            "low_success_rate": "ä½¿ç”¨ä¸åŒçš„æ‰§è¡Œç­–ç•¥æˆ–å·¥å…·",
            "llm_detected": anomaly.details.get("suggestion", "æ ¹æ®LLMå»ºè®®è°ƒæ•´æ‰§è¡Œç­–ç•¥") if anomaly.details else "ä½¿ç”¨å¤‡ç”¨æ‰§è¡Œæ–¹æ¡ˆ"
        }
        
        strategy = fallback_strategies.get(anomaly.fallback_type, "ä½¿ç”¨é»˜è®¤å›é€€ç­–ç•¥")
        logger.info(f"ğŸ”„ Suggested fallback strategy: {strategy}")
        return strategy
    
    def _update_performance_metrics(self, step_data: Dict, execution_time: float):
        """æ›´æ–°æ€§èƒ½æŒ‡æ ‡"""
        self.performance_metrics.update({
            "total_steps": len(self.execution_history),
            "avg_execution_time": (self.performance_metrics.get("avg_execution_time", 0) + execution_time) / 2,
            "last_step_time": execution_time,
            "success_rate": self._calculate_success_rate()
        })
    
    def _calculate_success_rate(self) -> float:
        """è®¡ç®—æˆåŠŸç‡"""
        if not self.execution_history:
            return 1.0
        
        success_count = sum(1 for step in self.execution_history if step.get("data", {}).get("success", True))
        return success_count / len(self.execution_history)
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æ‘˜è¦"""
        return {
            "total_steps": len(self.execution_history),
            "consecutive_failures": self.consecutive_failures,
            "success_rate": self._calculate_success_rate(),
            "performance_metrics": self.performance_metrics
        }
    
    def reset(self):
        """é‡ç½®ç›‘æ§å™¨çŠ¶æ€"""
        self.execution_history = []
        self.performance_metrics = {}
        self.step_start_time = None
        self.consecutive_failures = 0
        logger.info("ğŸ”„ ExecutionMonitor reset")

